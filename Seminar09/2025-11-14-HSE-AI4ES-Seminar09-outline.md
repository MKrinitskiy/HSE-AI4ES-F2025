# HSE-AI4ES: Семинар 9

## Тема: Прогноз временного ряда с использованием ансамблевых моделей и нейросетей

---

## 1. Цели семинара
- Восстановить и продолжить работу с ноутбуком из предыдущего семинара по прогнозу временного ряда.
- Использовать ранее подготовленные временные признаки (tsfresh) и применить к задаче новые модели:
  - Random Forest Regressor  
  - Gradient Boosting Regressor / CatBoost Regressor  
  - Многослойный персептрон (MLP Regressor)
- Сравнить качество моделей и понять их поведение на задаче краткосрочного прогноза.

---

## 2. Подготовка данных
### 2.1. Восстановление ноутбука
- Загрузка или пересоздание датасета с временными признаками.
- Уточнение: прогнозируем температурный ряд с горизонтом +1 час.

### 2.2. Ограничение размера выборки
- Для предотвращения переполнения памяти ограничиваем датасет до ~10 000 временных фрагментов.

### 2.3. Разделение признаков и целевой переменной
- Формирование `X` из признаков временных окон (lag-признаки, агрегаты и признаки tsfresh).
- Формирование `y` как значения ряда на следующий час.

### 2.4. Сохранение промежуточных результатов
- Сериализация рассчитанных признаков и `y` через `pickle` для ускорения дальнейшей работы.
- Возможность загрузки подготовленных массивов без повторного вычисления tsfresh.

### 2.5. Борьба со скоррелированными признаками
- Применение PCA:
  - `n_components = 4` (примерное значение)
  - Получение ортогональных признаков.
  - Проверка корреляционной матрицы (должна быть диагональной).

---

## 3. Разбиение на тренировочную и тестовую выборки
- Правильное разбиение для временных рядов — по блокам (например, неделям).
- Использование номера недели (`dt.isocalendar().week`) для построения:
  - списка недель,
  - случайного выбора недель в train/test,
  - разбиения X и y по неделям, а не по отдельным точкам.
- Итог: тренировка ≈ 70%, тест ≈ 30% объектов, организованных по недельным блокам.

---

## 4. Применение новых моделей
### 4.1. Random Forest
- Импорт: `RandomForestRegressor`.
- Обучение стандартным методом `fit`.
- Быстрое вычисление, высокая устойчивость.

### 4.2. Gradient Boosting и CatBoost
- CatBoost устанавливается через `pip install catboost`.
- Импорт: `CatBoostRegressor`.
- Особенности:
  - склонность к переобучению,
  - высокая точность,
  - медленнее обучается.

### 4.3. Нейросеть MLP
- Использование `MLPRegressor` из `sklearn.neural_network`.
- Настройка скрытых слоёв: например, `(100, 100, 100)`.
- Функция активации ReLU (по умолчанию).
- Обучение и первичная оценка качества.

---

## 5. Оценка качества моделей
### 5.1. Метрики
- MSE и RMSE для тестовой выборки.
- Сравнение результатов моделей с линейной регрессией (baseline).

### 5.2. Проверка переобучения
- Анализ MSE/RMSE на train и test.
- Если существенный разрыв → признаки переобучения.

### 5.3. Обсуждение результатов
- Линейная регрессия даёт surprisingly высокое качество на горизонте +1 час (инерционный характер ряда).
- MLP может переобучаться.
- Нужна оценка доверительных интервалов для строгих выводов.

---

## 6. План на следующий семинар
- Реализовать бутстрап-оценку неопределённости:
  - обучить 100–500 моделей каждого типа,
  - получить распределение метрик на train и test,
  - построить доверительные интервалы,
  - сравнить пересечения интервалов и степень переобучения.
- Построить доверительные интервалы для:
  - линейной регрессии,
  - MLP,
  - Random Forest,
  - CatBoost,
  - инерционной модели.
