## Лекция №9: Линейная регрессия и основы оптимизации параметрических моделей



1. **Организационные замечания.**

   — Преподаватель напомнил, что текущая задача носит демонстрационный характер — простая аппроксимация временного ряда (изменение веса во времени).

2. **Возврат к предпосылкам линейной регрессии.**

   — Независимость и одинаковое распределение наблюдений (i.i.d.).

   — Линейность связи между признаками и параметром распределения целевой переменной.

   — Нормальность остатков.

   — Гомоскедастичность (равенство дисперсий).

   → Обсуждение того, почему эти предпосылки нарушаются для временных рядов и почему линейную регрессию формально нельзя использовать в таких случаях.

3. **Формулировка линейной регрессии в матричной форме.**

   — $$ Y = X\theta + \varepsilon.$$

   — Объяснение смысла коэффициентов θ: смещение (bias) и наклон.

   — Добавление фиктивного признака (столбца единиц) в матрицу X.

4. **Минимизация функции потерь.**

   — Функция потерь:
   $$
   L(\theta) = (Y - X\theta)^T (Y - X\theta)
   $$
   .

   — Введение понятия градиента и условия экстремума $$\nabla_\theta L = 0$$.

   — Решение в аналитической форме:

   $$\theta^* = (X^TX)^{-1}X^TY$$

   — Обсуждение смысла «аналитического решения» и связи с теорией оптимизации.

5. **Визуализация ландшафта функции потерь.**

   — Демонстрация поверхности типа «параболическая чаша».

   — Интерпретация координат: θ₀, θ₁ и значение функции потерь.

   — Обсуждение положительной определённости гессиана как признака минимума.

6. **Практическая реализация в Python (NumPy).**

   — Пошаговое вычисление аналитического решения через транспонирование, матричное произведение и обращение матриц.

   — Интерпретация коэффициентов (пример: вес = 80 кг, наклон ≈ –1.1·10⁻⁷ кг/с).

   — Обсуждение смысла аппроксимации: модель не воспроизводит процесс точно, а минимизирует ошибку в среднем.

7. **Ограничения и бизнес-логика.**

   — Введение понятия постобработки: введение физических или логических ограничений на результат (например, «вес не может быть отрицательным»).

   — Упоминание о возможности добавления таких ограничений в *pipeline*.

8. **Использование реализации из Scikit-Learn.**

   — Обсуждение метода .fit() как процедуры подбора параметров модели.

   — Сравнение с ручной реализацией.

   — Вычисление RMSE на тренировочных данных (~0.7 кг).

9. **Полиномиальная регрессия.**

   — Введение новых признаков с помощью PolynomialFeatures до 5-й (позже 6-й) степени.

   — Обсуждение увеличения размерности X и θ.

   — Демонстрация, что модель остаётся линейной по параметрам, но нелинейной по исходным признакам.

   — Улучшение метрики RMSE (≈0.58 кг).

   — Обсуждение переобучения при чрезмерной степени полинома.

10. **Завершение и переход.**

    — Подчёркнуто, что линейная регрессия используется как учебный пример для понимания основ оптимизации и возникновения неопределённости моделей.