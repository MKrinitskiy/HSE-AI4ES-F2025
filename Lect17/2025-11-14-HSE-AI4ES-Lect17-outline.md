# HSE-AI4ES: Лекция 17

## Тема: Ансамбли моделей (stacking, bagging, random subspace, Random Forest)

### 1. Введение
- Краткое напоминание о деревьях решений и их слабых местах.
- Почему одиночные деревья не стоит применять как финальную модель.
- Понятие табличных/векторных данных: объекты-признаки.
- Автокорреляция в пространственных и временных данных — почему это усложняет задачи.

### 2. Ансамбли моделей: мотивация
- Аналогия с ансамблями атмосферных и климатических моделей.
- Почему усреднение результатов нескольких моделей может давать более устойчивый и точный результат.
- Эффект снижения дисперсии и неопределённости.

### 3. Агрегирование результатов моделей
- Регрессия: простое и взвешенное осреднение.
- Классификация: простое и взвешенное голосование.
- Идея объединения нескольких уже обученных моделей.

### 4. Стекинг (Stacking)
- Принцип: базовые модели → метапризнаки → метамодель.
- Как формируются метапризнаки (оценки вероятностей, предсказания).
- Использование K-fold для корректного построения метапризнаков.
- Требования к метамодели: способность работать со скоррелированными признаками.
- Где стекинг полезен, а где — избыточен.

### 5. Метод случайных подпространств (Random Subspaces)
- Зачем увеличивать разнообразие моделей.
- Обучение одинаковых или разных моделей на разных подмножествах признаков.
- Использование при построении ансамблей.

### 6. Bootstrap и bagging
- Понятие Bootstrap: выборка с возвращением.
- Свойства bootstrap-подвыборок: идентичность распределения исходным данным.
- Обучение нескольких моделей на разных bootstrap-подвыборках.
- Простое осреднение/голосование как основной метод агрегирования.
- Почему bagging работает даже с «слабыми» моделями (например, деревьями).

### 7. Дисперсия и зависимость между базовыми моделями
- Формула дисперсии усреднённого предсказания.
- Влияние количества моделей K.
- Роль попарной корреляции ρ.
- Не интуитивный вывод: базовые модели должны быть *разнообразными*.

### 8. Random Forest — случайный лес
- Интуитивное объяснение: большое количество сильно различающихся деревьев.
- Основные идеи:
  - bootstrap-подвыборки объектов,
  - выбор случайного подмножества признаков при обучении каждого дерева,
  - выбор случайного подмножества признаков на каждом этапе разбиения (каждом узле).
- Почему деревья обучают «до конца», без ограничения глубины.
- Получение некоррелированных деревьев и снижение дисперсии ансамбля.

### 9. Свойства и преимущества Random Forest
- Устойчивость к выбросам.
- Неприхотливость к пропущенным значениям (категория “missing”).
- Сложно переобучить (в отличие от многих моделей).
- Возможность бесплатной оценки качества на out-of-bag выборке.
- Простота применения и высокая точность на табличных данных.

### 10. Ограничения и недостатки
- Трудности интерпретации (но возможна оценка важности признаков).
- Вычислительная нагрузка при обучении большого ансамбля.
- Потеря качества, если слишком ограничивать глубину деревьев.
- Невозможность извлечь гладкие зависимости (в отличие от линейных/нейросетей).

### 11. Выводы
- Ансамбли — один из самых эффективных подходов на табличных данных.
- Random Forest — базовый и очень мощный инструмент анализа.
- Стекинг полезен лишь при наличии сильных разнообразных моделей.
- Bagging и случайные подпространства остаются фундаментальными блоками ансамблей.

