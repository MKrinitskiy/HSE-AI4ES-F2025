# HSE-AI4ES: Лекция 18

## Тема: Градиентный бустинг, CatBoost, обобщённые линейные модели и введение в искусственные нейронные сети

---

## 1. Разминка и рефлексия
- Краткое повторение принципов Random Forest:
  - Bootstrap‑выборки и random subspace.
  - Простое осреднение/голосование как способ агрегации.
  - Идея снижения дисперсии ансамбля.

---

## 2. Переход к бустингу
### 2.1. Ограничения бэггинга и мотивация
- Бэггинг не использует информацию о систематических ошибках базовых моделей.
- Идея бустинга: не осреднять, а последовательно улучшать модель, компенсируя её ошибки.

### 2.2. Интуиция бустинга
- Строим первое дерево решений → оцениваем ошибку.
- Новое дерево обучается на ошибке предыдущего ансамбля.
- Итерационная сумма деревьев с малым коэффициентом (learning rate).

### 2.3. Формальная схема градиентного бустинга
- Ошибка текущей композиции интерпретируется как отрицательный градиент функции потерь.
- Новое дерево аппроксимирует этот градиент.
- Обновление ансамбля:
  - *F_{m+1}(x) = F_m(x) + η · h_m(x)*
- Отсутствие bootstrap‑семплирования и случайных подпространств.

### 2.4. Adaboost (кратко)
- Адаптивное взвешивание объектов.
- Историческая модель бустинга, редко применяемая сейчас.

---

## 3. Современные реализации градиентного бустинга
### 3.1. Популярные библиотеки
- XGBoost.
- LightGBM.
- CatBoost (рекомендованная реализация).

### 3.2. Почему CatBoost:
- Быстрая и стабильная реализация.
- Поддержка GPU.
- Хорошая работа с категориальными признаками.
- Высокая точность во многих прикладных задачах.

---

## 4. Свойства градиентного бустинга
- Склонность к переобучению при большом числе деревьев.
- Необходимость строгого контроля качества по валидационной выборке.
- Многочисленные гиперпараметры:
  - Глубина деревьев.
  - Learning rate.
  - Количество деревьев.
  - Параметры регуляризации.
- Вычислительная цена:
  - Длительное обучение на больших данных.
  - Оптимизация гиперпараметров требует ресурсов, особенно без GPU.

---

## 5. Сравнение Random Forest и бустинга
- Random Forest:
  - Практически не переобучается.
  - Хорош для быстрого и устойчивого baseline‑решения.
- Градиентный бустинг:
  - Более выразительный.
  - Выбор по умолчанию для максимизации качества.
  - Требует больше вычислительных ресурсов и осторожной настройки.

---

## 6. Обобщённые линейные модели (GLM)
### 6.1. Связь с линейной и логистической регрессией
- Общая схема: параметр распределения = функция активации(θᵀx).
- Примеры:
  - Линейная регрессия: тождественная активация.
  - Логистическая регрессия: сигмоид.
  - Мультиномиальная логистическая регрессия: softmax.

### 6.2. Функция активации и функция потерь
- Вид функции потерь определяется распределением, а не активацией.
- Активация задаёт допустимый диапазон параметра распределения.

---

## 7. Обобщённые аддитивные модели (GAM)
- Идея: применять произвольные функции к отдельным признакам.
- Увеличение выразительной способности без изменения базовой логики GLM.
- Ограничения: отсутствие взаимодействий между признаками.

---

## 8. Мотивация перехода к нейросетям
- GLM → GAM → «расширенные» конструкции.
- Идея: обучать преобразования признаков автоматически, не задавая вручную.
- Искусственные нейронные сети как обобщение GLM.

---

## 9. Искусственные нейронные сети (ANN)
### 9.1. Структура MLP
- Полносвязанная сеть: слои, нейроны, параметры.
- Скрытые слои и скрытые представления.
- Функции активации:
  - На скрытых слоях — любые монотонные (ReLU, sigmoid, tanh).
  - На выходе — зависит от задачи (identity, sigmoid, softmax).

### 9.2. Принцип вычисления
- Последовательные преобразования: θᵀx → активация → следующий слой.
- Матричная форма представления параметров.
- Связь с обобщёнными линейными моделями.

### 9.3. Обучение
- Общая схема как у параметрических моделей:
  - Функция потерь (MSE или cross‑entropy).
  - Градиентная оптимизация (backpropagation + SGD/Adam).
- Автоматическое дифференцирование.

### 9.4. Выбор архитектуры
- Глубина (3–7 слоёв) и ширина (50–200 нейронов) как разумные стартовые значения.
- Глубина повышает выразительность эффективнее, чем ширина.

---

## 10. Итоги и переход к семинару
- Деревья решений — не самостоятельная боевая модель.
- Random Forest — быстрый и устойчивый инструмент.
- Градиентный бустинг — наиболее сильный метод на табличных данных.
- GLM и GAM — необходимый теоретический фундамент.
- Нейросети — универсальный дифференцируемый аппарат представления данных.
- Дальнейшая работа: практическое обучение MLP.
