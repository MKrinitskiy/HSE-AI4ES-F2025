# HSE-AI4ES: Лекция 18

## Градиентный бустинг.

## Обобщенные линейные модели -> обобщенные аддитивные модели -> Искусственные нейронные сети.

В этой лекции рассматривается эволюция подходов к построению моделей на табличных данных — от бэггинга и случайных лесов к современным методам градиентного бустинга. Объясняется, почему простое осреднение базовых алгоритмов не всегда позволяет эффективно использовать информацию об ошибках модели, и как последовательное исправление этих ошибок формирует основу бустинговых алгоритмов.

Отдельное внимание уделено практическим реализациям бустинга, включая CatBoost, XGBoost и LightGBM. Рассматриваются их преимущества, ограничения, чувствительность к гиперпараметрам и вычислительная стоимость. Подчёркивается, что бустинг часто оказывается наиболее точным методом для задач с векторными признаками, но требует осторожной настройки, чтобы избежать переобучения.

Вторая часть лекции посвящена переходу от классических параметрических моделей к искусственным нейронным сетям. Через обобщённые линейные и аддитивные модели показывается логическая связь между линейной регрессией, логистической регрессией и многослойными персептронами. Формируется базовое понимание того, как устроены скрытые представления и почему нейросети являются универсальным инструментом для построения сложных аппроксимаций.
