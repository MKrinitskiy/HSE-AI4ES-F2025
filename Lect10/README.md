### Лекция №10: Выразительная способность модели, переобучение и неопределённость параметров

1. **Введение и связь с предыдущей лекцией.**

   — Лекция началась с напоминания, что полиномиальная регрессия остаётся *линейной моделью* — линейной по параметрам θ.

   — Цель занятия — рассмотреть влияние степени полинома на качество модели и понять, как возникает переобучение и неопределённость.

2. **Повышение степени полинома и численные ошибки.**

   — Показана зависимость RMSE от степени полинома (до 25-й).

   — До ≈9-й степени ошибка уменьшается, затем резко растёт из-за *численных проблем* (переполнения float64 при огромных значениях признаков).

   — Объяснено: большие различия в порядке величин приводят к потере точности при обращении матриц.

   — Основная причина — отсутствие нормализации признаков.

3. **Роль нормализации признаков.**

   — Предложено решение: нормировать признаки до единичного масштаба (кроме константной единицы).

   — Нормализация устраняет дисбаланс и снижает численные ошибки.

   — Обсуждение, что нормализация обязательна для всех параметрических моделей (линейная, логистическая регрессия, нейросети и др.).

4. **Переобучение и выразительная способность модели.**

   — Обсуждение понятия *выразительности (expressive power)*.

   — Гиперпараметр «степень полинома» регулирует сложность модели.

   — При увеличении степени модель лучше описывает тренировочные данные, но хуже — тестовые.

   — Сформулированы понятия:

   

   - *Переобучение* — качество на обучающей выборке высокое, на тестовой резко падает.

   - *Недообучение* — низкое качество и на обучающей, и на тестовой.

   - *Обобщающая способность* — умение модели работать на новых данных.

     — Закреплено правило: **качество модели нельзя оценивать на обучающей выборке.**

   

5. **Эксперимент с синтетическими данными.**

   — Рассмотрена двумерная задача с признаками x₀ и x₁ и линейной зависимостью целевой переменной.

   — Показано, что даже в идеальных условиях линейная регрессия занижает дисперсию целевой переменной — модели «слишком уверены».

6. **Оценка качества на тренировочных и тестовых выборках.**

   — 10 000 экспериментов с обучением модели на разных подвыборках.

   — Построены гистограммы MAE (среднего модуля отклонения) для train и test.

   — В большинстве случаев ошибка на тестовых данных выше, но не всегда (≈60 % случаев).

   — Обсуждение причин редких «аномальных» ситуаций, когда на тесте ошибка меньше, чем на трейне (синтетические данные, идеальные предпосылки).

7. **Влияние масштаба признаков на неопределённость качества.**

   — Продемонстрировано: при различии масштабов признаков (до e¹⁵) дисперсия метрики качества увеличивается.

   — Следствие: нормализация уменьшает разброс качества и повышает устойчивость модели.

8. **Неопределённость параметров и целевой переменной.**

   — Показано, что при обучении множества моделей на разных подвыборках параметры θ имеют распределение.

   — С увеличением размера обучающей выборки дисперсия параметров уменьшается, затем выходит на насыщение.

   — Следствие: неопределённость параметров порождает неопределённость в прогнозе целевой переменной.

9. **Доверительный интервал и оценка неопределённости.**

   — Разобран принцип эмпирического построения доверительных интервалов (например, 95 %):

   

   - либо по квантилям эмпирического распределения,

   - либо при нормальности — по σ и табличным квантилям.

     — Обсуждение связи с практикой публикаций: в научных статьях необходимо указывать не только значение (например, 3 °C), но и его неопределённость («± 0.2 °C при 95 % доверии»).

   