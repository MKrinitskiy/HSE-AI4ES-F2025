- # Лекция 3. Постановка задач машинного обучения

  ### 1. Напоминание предыдущего материала  
  - Задачи восстановления (регрессия) и классификации.  
  - Тривиальные модели (константная, случайная).
  
  ### 2. Базовые понятия  
  - Объекты и события как единицы анализа.  
  - Признаковое описание: релевантность признаков, выбор признаков (напр. тенденция vs мгновенное значение).  
  - Случайная природа признаков и целевой переменной; источники шума (естественные, инструментальные, несинхронность).  
  - Целевая переменная \(Y\) как особый признак, но подчинённый тем же законам случайной величины.
  
  ### 3. Структура пространства признаков  
  - Типы признаков: числовые (непрерывные) и категориальные.  
  - Пространство признаков и множество значений целевой переменной.  
  
  ### 4. Обучение с учителем  
  - Две основные задачи: регрессия и классификация.  
  
  - Модель как отображение 
    $$
    f: \mathcal{X} \to \mathcal{Y}
    $$
  
  ### 5. Обучение без учителя  
  - Определение и смысл неконтролируемого обучения (unsupervised).  
  - **Кластеризация**:  
    - Возможные тривиальные решения: один кластер или каждый объект отдельным кластером.  
    - Отсутствие единственного «правильного» решения.  
    - Необходимость меры качества (например, коэффициент силуэта).  
    - Примеры методов: K-means, DBSCAN.
  
  ### 6. Снижение размерности  
  - Проблема визуализации многомерных данных (напр. любые данные, заданные на пространственной сетке).  
  - Цели для снижения размерности:  
    - визуализация,  
    - сокращение вычислительной нагрузки,  
    - борьба с «проклятием размерности»,  
    - выделение информативных структур / подавление шума.  
  - Понятие проклятия размерности: искажение геометрии в высоких измерениях, уменьшение «контраста» расстояний.  
  - Метрики расстояния: Евклидова, Минковского.
  
  ### 7. Другие постановки задач  
  - Аппроксимация распределения данных (будет подробно в отдельной лекции).  
  - Обучение с частичным привлечением учителя (semi-supervised): псевдометки, гибрид кластеризации + классификации.  
  - Обучение с подкреплением (reinforcement learning): понятия агента, среды, награды / штрафа.  
  - Самоконтролируемое обучение (self-supervised).  
  - Выучивание метрики (metric learning) как подход, когда стандартные расстояния не «работают».
  
  ### 8. Практический вывод  
  - Ключевые вопросы постановки любой задачи:  
    1. Что у нас есть на руках (данные, признаки)?  
    2. Что мы хотим получить (целевая переменная, структура данных)?  
    3. Как будем измерять качество (метрика, критерий)?  
  - Роли исследователя: постановщик задачи и (частично) исполнитель.

  ---

  

  ## Примечания: «эффект апельсиновой корки» и проклятие размерности
  
  В высокоразмерных пространствах наша интуиция из 2D или 3D перестаёт работать.  
  - **Проклятие размерности** означает, что с ростом числа признаков:  
    - точки становятся все более «разреженными» (для покрытия пространства требуется экспоненциально больше данных),  
    - расстояния между точками «выравниваются» (ближайший и дальний сосед оказываются почти на одинаковом расстоянии),  
    - методы, основанные на евклидовом расстоянии как мере различия между объектами (кластеризация, поиск ближайших соседей), начинают работать плохо.  
  - **Эффект апельсиновой корки** показывает, что почти весь объём высокоразмерного шара сосредоточен в тонком слое возле поверхности: точки оказываются «на кожуре», а центр шара почти пуст.
    - Интересное нетривиальное следствие: для высокоразмерной случайной величины, распределенной нормально, основная масса распределения оказывается вовсе не в области начала координат.


  ### Первые источники  
  - Richard Bellman, *Dynamic Programming*, Princeton University Press, 1957 — впервые введён термин *curse of dimensionality*.  
  - Frank Nielsen, *The Curse of Dimensionality: Inside Out*, Springer, 2018 ([ResearchGate PDF](https://www.researchgate.net/publication/327498046_The_Curse_of_Dimensionality_Inside_Out?utm_source=chatgpt.com)) — современный обзор.  

  ### Современные материалы  
  - Wikipedia: [Curse of Dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality?utm_source=chatgpt.com).  
  - Yann Dubs, *Machine Learning Glossary* (обзор на простом языке): [yanndubs.github.io](https://yanndubs.github.io/machine-learning-glossary/concepts/curse?utm_source=chatgpt.com).  
  - Arxiv: *Interpreting the Curse of Dimensionality from Distance Concentration and Manifold Effect* ([arxiv.org/abs/2401.00422](https://arxiv.org/abs/2401.00422?utm_source=chatgpt.com)) — о современных интерпретациях.  

  ### Об эффекте «апельсиновой корки»  
  - Математическое объяснение: объём оболочки \( \varepsilon \) в \(n\)-мерном шаре радиуса \(r\) равен  
    \[
    1 - \left(\frac{r - \varepsilon}{r}\right)^n.
    \]
    При больших \(n\) эта доля быстро стремится к 1.  

---