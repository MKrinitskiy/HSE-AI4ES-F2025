## Лекция №12

**Основная тема:**

*Переход от задач регрессии к задачам классификации. Градиентная оптимизация, неопределенность параметров и базовые методы классификации.*

### 1. Введение: градиентная оптимизация как универсальный подход**



- Обсуждены отличия **аналитического решения** (линейная регрессия) и **градиентной оптимизации**.
- Введено понятие **гиперпараметра оптимизации** — прежде всего *learning rate* и *tolerance* (эпсилон в условии остановки).
- Отмечено, что даже линейная регрессия в варианте **Ridge Regression** решается градиентными методами.
- Показано, что основа любой оптимизации — **матричные операции**; искусственный интеллект в целом основан на эффективных матричных вычислениях.



### **2. Эксперимент: неопределенность параметров при градиентной оптимизации**

**Цель:** показать, как неопределенность в параметрах модели зависит от свойств данных.

**Сценарии:**

1. **Идеальный случай:** признаки некоррелированы, одинаковый масштаб.

   → Траектории оптимизации сходятся в один минимум, разброс параметров мал.

2. **Сильно скоррелированные признаки:**

   → Функция потерь вытянута, напоминает «утятницу»; разброс параметров велик, неопределенность возрастает.

   → Рекомендация: удалять коррелированные признаки или применять **PCA (метод главных компонентов)**.

3. **Сильно различающиеся масштабы признаков:**

   → Появляется вытянутость вдоль одной оси, неопределенность параметров возрастает даже при нулевой корреляции.

   → Рекомендация: обязательно **нормировать признаки** (использовать *StandardScaler*).

**Вывод:**

Неопределенность параметров в параметрических моделях возникает не только из-за шума данных, но и из-за свойств входных признаков.

→ Для параметрических моделей обязательна нормализация и устранение коррелированных признаков.



### **3. Переход к задачам классификации**

**Постановка задачи:**



- Дано множество объектов (вектор признаков X) и их классы (Y).
- Требуется построить модель, присваивающую новым объектам метку класса.

**Примеры задач:**

- Определение спам/не спам.
- Определение наличия циклона по полям атмосферы.
- Определение объектов на спутниковых изображениях (морские млекопитающие и т.п.).

**Обсуждены базовые элементы:**

- Объекты → кусочки поверхности океана.
- Признаковое описание → числовые значения каналов RGB.
- Целевая переменная → наличие/отсутствие объекта (0/1).
- Мера качества → доля верных ответов (*accuracy*), с оговоркой о различии между *accuracy* и *precision*.



### **4. Метод k ближайших соседей (k-NN)**

- Интуиция метода: класс нового объекта определяется по меткам ближайших соседей в пространстве признаков.
- Рассмотрено понятие **близости (distance metric)** — чаще всего Евклидово расстояние; однако при росте размерности теряет смысл (проклятие размерности).
- Введён **гиперпараметр k** — количество соседей.

**Два варианта голосования:**

1. Простое голосование (majority vote).
2. Взвешенное голосование — использование весов классов или отдельных объектов.

**Приведены формулы:**

- Для классификации (majority vote / weighted vote).
- Для регрессии (усреднение / взвешенное усреднение).

**Недостатки метода:**

- При большом числе данных вычислительно затратен (нужно считать расстояния до всех объектов).
- Плохо работает при высокой размерности.



### **5. Вероятностный подход: Байесовская классификация**

- Переход к **вероятностной интерпретации классификации**.

- Напомнена формула Байеса:
  $$
  P(\theta|x) = \frac{P(x|\theta)P(\theta)}{P(x)}
  $$
  

- Введена идея: знаменатель одинаков для всех классов, поэтому достаточно сравнивать числитель.

- Показано на примере задачи с двумя нормальными распределениями (для каждого класса):

  вычисление вероятностей принадлежности объекта классу A или B.

- Введено различие между:

  

  - **Байесовским классификатором** (если известны истинные распределения признаков);
  - **Наивным байесовским классификатором** (аппроксимация с допущением независимости признаков).

  

**Интуиция:**

Вероятность класса меняется от 0 до 1 вдоль оси признака;

Вероятности классов суммируются в 1.



### **6. Итоги лекции**

- Повторены основные принципы градиентной оптимизации.

- Рассмотрено влияние корреляции и масштабов на устойчивость параметрических моделей.

- Введены базовые понятия задач **классификации**, включая метрики качества.

- Разобраны два метода классификации:

  

  1. k-NN (эвристический, неметрический).
  2. Байесовский классификатор (вероятностный).