## Лекция №11

Методы оптимизации параметрических моделей. Градиентный спуск на примере линейной регрессии.

**Основные блоки:**

1. **Повторение ключевых понятий**

   

   - Разграничение понятий *линейная регрессия* (модель) и *восстановление регрессии* (задача).
   - Напоминание о структуре признакового описания и целевой переменной.
   - Введение понятия *функции потерь* как меры качества аппроксимации.
   - Цель обучения — **минимизация функции потерь** по параметрам модели.

   

2. **Ландшафт функции потерь**

   

   - Визуализация функции потерь для двумерной задачи линейной регрессии.
   - Обсуждение зависимости формы ландшафта от данных: если данные изменяются, смещается минимум и оптимальные коэффициенты θ.

   

3. **Аналитическое решение**

   

   - Решение задачи линейной регрессии через приравнивание градиента функции потерь к нулю.
   - Объяснение понятия *градиент* как совокупности частных производных (вектор в пространстве параметров).
   - Для линейной регрессии градиент можно вычислить в матричной форме, что даёт аналитическое решение.

   

4. **Классы методов оптимизации**

   

   - **Аналитическая оптимизация** — применима для выпуклых функций, имеющих единственный минимум.
   - **Стохастические методы** (метод Монте-Карло): случайное сэмплирование точек в пространстве параметров.
   - **Эволюционные методы** — применимы даже при дискретных параметрах, но вычислительно дороги.
   - Переход к **градиентным методам** как базовому способу оптимизации параметрических моделей.

   

5. **Понятие градиента и антиградиента**

   

   - Градиент показывает направление наибольшего роста функции.
   - Для минимизации используется движение в противоположную сторону — по **антиградиенту**.
   - Аналогия с ветром и градиентом давления.

   

6. **Алгоритм градиентного спуска**

   

   - Пошаговое обновление параметров:

     \theta^{(t+1)} = \theta^{(t)} - \eta \nabla_\theta L

     где η — *learning rate* (темп обучения).

   - Итерации продолжаются до выполнения **условия остановки**:

     

     - малое изменение функции потерь;
     - малая длина градиента;
     - либо достижение максимального числа итераций.

     

   

7. **Практическая демонстрация**

   

   - Сравнение разных значений шага обучения:

     

     - малый шаг → медленная, но устойчивая сходимость (~400 итераций);
     - большой шаг → колебания и возможный **разброс (divergence)**.

     

   - Визуализация траектории спуска в пространстве параметров и графика эволюции функции потерь.

   

8. **Выводы**

   

   - Все параметрические модели, включая нейронные сети, обучаются методом градиентного спуска.
   - Ключевые гиперпараметры: шаг обучения, критерий остановки, начальное приближение.
   - На следующей лекции будет рассмотрено влияние масштабов признаков и корреляций на форму функции потерь и траекторию оптимизации.